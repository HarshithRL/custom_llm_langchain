{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in .\\.lease\\lib\\site-packages (1.44.0)\n",
      "Requirement already satisfied: langchain in .\\.lease\\lib\\site-packages (0.2.16)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in .\\.lease\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in .\\.lease\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\.lease\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in .\\.lease\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in .\\.lease\\lib\\site-packages (from openai) (2.9.0)\n",
      "Requirement already satisfied: sniffio in .\\.lease\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in .\\.lease\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in .\\.lease\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in .\\.lease\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\.lease\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\.lease\\lib\\site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in .\\.lease\\lib\\site-packages (from langchain) (0.2.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in .\\.lease\\lib\\site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in .\\.lease\\lib\\site-packages (from langchain) (0.1.116)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in .\\.lease\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in .\\.lease\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in .\\.lease\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in .\\.lease\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: certifi in .\\.lease\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in .\\.lease\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in .\\.lease\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\.lease\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in .\\.lease\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in .\\.lease\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in .\\.lease\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in .\\.lease\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.2)\n",
      "Requirement already satisfied: tzdata in .\\.lease\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.lease\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.lease\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in .\\.lease\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: colorama in .\\.lease\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\.lease\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openai langchain langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) designed to process, understand, and generate human-like language. They are a crucial component of natural language processing (NLP) and have numerous applications in areas like language translation, text summarization, sentiment analysis, and chatbots.\n",
      "\n",
      "**How LLMs Work:**\n",
      "\n",
      "LLMs are typically trained on vast amounts of text data, which enables them to learn patterns, relationships, and structures of language. This training data can come from various sources, including books, articles, websites, and online conversations. The models use complex algorithms to analyze and represent language in a way that allows them to generate coherent and contextually relevant text.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "1. **Scale:** LLMs are trained on massive datasets, which allows them to learn a wide range of language patterns and nuances.\n",
      "2. **Depth:** These models have many layers, enabling them to capture complex language structures and relationships.\n",
      "3. **Self-supervised learning:** LLMs can learn from raw text data without explicit human annotation or supervision.\n",
      "\n",
      "**Types of LLMs:**\n",
      "\n",
      "1. **Recurrent Neural Networks (RNNs):** These models use recurrent connections to process sequential data, such as text or speech.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# How to get your Databricks token: https://docs.databricks.com/en/dev-tools/auth/pat.html\n",
    "# DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "# Alternatively in a Databricks notebook you can use this:\n",
    "# DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "DATABRICKS_TOKEN = 'API key'\n",
    "client = OpenAI(\n",
    "  api_key=DATABRICKS_TOKEN,\n",
    "  base_url=\"base url\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "  messages=[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an AI assistant\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Tell me about Large Language Models\"\n",
    "  }\n",
    "  ],\n",
    "  model=\"databricks-meta-llama-3-1-405b-instruct\",\n",
    "  max_tokens=256\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\lease\\.lease\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in DatabricksOpenAIChatModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, ChatMessage, ChatResult\n",
    "\n",
    "class DatabricksOpenAIChatModel(BaseModel):\n",
    "    model_name: str\n",
    "    databricks_token: str\n",
    "    base_url: str\n",
    "    client: OpenAI = Field(default=None, exclude=True)  # Optional field, excluded from validation\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the client after pydantic does its initialization\n",
    "        self.client = OpenAI(\n",
    "            api_key=self.databricks_token,\n",
    "            base_url=self.base_url\n",
    "        )\n",
    "\n",
    "    def _convert_messages_to_openai_format(self, messages):\n",
    "        openai_messages = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                openai_messages.append({\"role\": \"user\", \"content\": message.content})\n",
    "            elif isinstance(message, AIMessage):\n",
    "                openai_messages.append({\"role\": \"assistant\", \"content\": message.content})\n",
    "            elif isinstance(message, ChatMessage):\n",
    "                openai_messages.append({\"role\": message.role, \"content\": message.content})\n",
    "        return openai_messages\n",
    "\n",
    "    def _generate(self, messages, stop=None):\n",
    "        # Convert the LangChain message format to OpenAI format\n",
    "        openai_messages = self._convert_messages_to_openai_format(messages)\n",
    "        \n",
    "        # Call the Databricks model using the OpenAI client\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=openai_messages,\n",
    "            model=self.model_name,\n",
    "            max_tokens=256\n",
    "        )\n",
    "        \n",
    "        # Extract the content of the AI's response\n",
    "        ai_response_content = response.choices[0].message.content\n",
    "        \n",
    "        # Return a ChatResult containing the response from the AI model\n",
    "        return ChatResult(generations=[AIMessage(content=ai_response_content)])\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"databricks-openai-chat-model\"\n",
    "\n",
    "    # Config class to allow arbitrary types like OpenAI client\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # Allow arbitrary types like the OpenAI client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Iterator\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor\n",
    "\n",
    "class DatabricksOpenAIChatModel(BaseChatModel):\n",
    "    model_name: str\n",
    "    databricks_token: str\n",
    "    base_url: str\n",
    "    client: OpenAI = Field(default=None, exclude=True)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the client after pydantic does its initialization\n",
    "        self.client = OpenAI(\n",
    "            api_key=self.databricks_token,\n",
    "            base_url=self.base_url\n",
    "        )\n",
    "\n",
    "    def _convert_messages_to_openai_format(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:\n",
    "        openai_messages = []\n",
    "        for message in messages:\n",
    "            if isinstance(message, HumanMessage):\n",
    "                openai_messages.append({\"role\": \"user\", \"content\": message.content})\n",
    "            else:\n",
    "                # Handle other message types if necessary\n",
    "                pass\n",
    "        return openai_messages\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        openai_messages = self._convert_messages_to_openai_format(messages)\n",
    "\n",
    "        # Call the Databricks model using the OpenAI client\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=openai_messages,\n",
    "            model=self.model_name,\n",
    "            max_tokens=256\n",
    "        )\n",
    "\n",
    "        # Extract the content from the response\n",
    "        ai_message_content = response.choices[0].message.content\n",
    "\n",
    "        # Create ChatGeneration object\n",
    "        chat_generation = ChatGeneration(\n",
    "            message=AIMessage(\n",
    "                content=ai_message_content,\n",
    "                additional_kwargs={},  # Additional payload if needed\n",
    "                response_metadata={\"time_in_seconds\": 3}  # Example metadata\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return ChatResult(generations=[chat_generation])\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        # Streaming is not implemented in this example\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"databricks-openai-chat-model\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import ChatResult\n",
    "\n",
    "\n",
    "DATABRICKS_TOKEN = 'dapi28e6733ec062accc15888fac844b489a-3'\n",
    "BASE_URL = \"https://adb-7488500312027058.18.azuredatabricks.net/serving-endpoints\"\n",
    "\n",
    "databricks_model = DatabricksOpenAIChatModel(\n",
    "    model_name=\"databricks-meta-llama-3-1-405b-instruct\", \n",
    "    databricks_token=DATABRICKS_TOKEN,\n",
    "    base_url=BASE_URL\n",
    ")\n",
    "\n",
    "# Define the messages for the chat model\n",
    "messages = [\n",
    "    HumanMessage(content=\"Tell me about Large Language Models\")\n",
    "]\n",
    "\n",
    "# Generate the response\n",
    "response = databricks_model._generate(messages)\n",
    "response: ChatResult = databricks_model.invoke(messages)\n",
    "\n",
    "# print(response.generations[0].content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Large Language Models (LLMs) are a type of artificial intelligence (AI) model that has revolutionized the field of natural language processing (NLP). They are designed to process and understand human language at an unprecedented scale and complexity. Here's an overview:\\n\\n**What are Large Language Models?**\\n\\nLLMs are a type of deep learning model that uses neural networks to learn patterns and relationships in language data. They are trained on massive datasets of text, typically hundreds of billions of words, to generate predictive models that can complete tasks such as language translation, text summarization, question answering, and even text generation.\\n\\n**How do Large Language Models work?**\\n\\nLLMs work by using a technique called masked language modeling. The model is trained on a large corpus of text, where some of the words are randomly replaced with a special token (e.g., [MASK]). The model then predicts the original word that was replaced, based on the context provided by the surrounding words. This process is repeated millions of times, allowing the model to learn the patterns and relationships in language.\\n\\n**Characteristics of Large Language Models**\\n\\n1. **Scalability**: LLMs are designed to handle massive amounts of text data, often hundreds of billions of words.\\n2. **Complexity**:\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'time_in_seconds': 3}, id='run-172ff053-40cd-4a53-85f9-ecfae757bf5c-0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databricks_model.invoke('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in .\\.lease\\lib\\site-packages (1.8.0.post1)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in .\\.lease\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in .\\.lease\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in .\\.lease\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\.lease\\lib\\site-packages (from langchain_community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\.lease\\lib\\site-packages (from langchain_community) (3.10.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in .\\.lease\\lib\\site-packages (from langchain_community) (0.2.16)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in .\\.lease\\lib\\site-packages (from langchain_community) (0.2.38)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in .\\.lease\\lib\\site-packages (from langchain_community) (0.1.116)\n",
      "Requirement already satisfied: requests<3,>=2 in .\\.lease\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in .\\.lease\\lib\\site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in .\\.lease\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in .\\.lease\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in .\\.lease\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (2.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\.lease\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in .\\.lease\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\.lease\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in .\\.lease\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.lease\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.lease\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.lease\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.lease\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in .\\.lease\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: anyio in .\\.lease\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in .\\.lease\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in .\\.lease\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in .\\.lease\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\.lease\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in .\\.lease\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in .\\.lease\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2.23.2)\n",
      "Requirement already satisfied: tzdata in .\\.lease\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2024.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.3 MB 660.6 kB/s eta 0:00:04\n",
      "    --------------------------------------- 0.0/2.3 MB 660.6 kB/s eta 0:00:04\n",
      "    --------------------------------------- 0.0/2.3 MB 660.6 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/2.3 MB 525.1 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.2/2.3 MB 841.6 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.3 MB 831.5 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.7/2.3 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 3.9 MB/s eta 0:00:00\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.16 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "# import google.gene/rativeai as genai\n",
    "# from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from dotenv import load_dotenv\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me about {topic}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=databricks_model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarshithR\\AppData\\Local\\Temp\\ipykernel_30028\\1356006775.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm_chain.run(topic=\"Large Language Models\")\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.run(topic=\"Large Language Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Large Language Models (LLMs) are a class of artificial intelligence (AI) models that have gained significant attention in recent years. Here's an overview:\\n\\n**What are Large Language Models?**\\n\\nLLMs are a type of deep learning model designed to process and generate human-like language. They are trained on vast amounts of text data, typically from the internet, books, and other digital sources. These models learn to predict the next word or character in a sequence, given the context of the surrounding words. This prediction task allows them to develop an understanding of language patterns, grammar, syntax, and semantics.\\n\\n**Key Characteristics:**\\n\\n1. **Size**: LLMs are massive models with billions of parameters (weights and biases). This size enables them to capture complex relationships between words and phrases.\\n2. **Training data**: These models are trained on enormous datasets, often consisting of trillions of tokens (words or characters).\\n3. **Self-supervised learning**: LLMs learn from raw text data without explicit supervision or labeled training data.\\n4. **Transfer learning**: LLMs can be fine-tuned for specific tasks, such as language translation, question-answering, or text generation, using smaller amounts of labeled data.\\n\\n**Applications:**\\n\\n1. **\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
